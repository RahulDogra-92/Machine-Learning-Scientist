{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa4d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pdfplumber\n",
    "import torch\n",
    "import spacy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1bc6c3",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f18cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def clean(data):\n",
    "    all_text1 = BeautifulSoup(data).get_text()\n",
    "    all_text1 = decontracted(all_text1)\n",
    "    #new_data1[sentance] = re.sub(\"\\S*\\d\\S*\", \"\", new_data1[sentance]).strip() #to remove nos\n",
    "    #new_data1[sentance] = re.sub('[^A-Za-z]+', ' ', new_data1[sentance])\n",
    "    all_text1 = re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", all_text1) # emails\n",
    "    all_text1 = re.sub(r'[^\\w ]+', \"\", all_text)\n",
    "    all_text1 = re.sub(r'[A-Za-z]+[\\d]+[\\w]*|[\\d]+[A-Za-z]+[\\w]*',\" \",all_text1) # for alphanummerical\n",
    "    all_text1 = re.sub(r'[_]',\" \",all_text1) #remove underscore\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    all_text1 = ' '.join(e.lower() for e in all_text1.split() if e.lower() not in stopwords)\n",
    "    all_text1.strip()\n",
    "    return all_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cbd9d",
   "metadata": {},
   "source": [
    "## Conversion of PDFS into TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9627864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFA_ZRG Partners LLC_v0.1_LB_20211007 edited_LB_20211021  (Final - TKJ Accepted1).pdf\n",
      "SUPP-00002602 - CURIOUS LION SFA SUPPLIER FRAMEWORK AGREEMENT MSA.pdf\n"
     ]
    }
   ],
   "source": [
    "path = \"D:\\\\OneDrive_2022-01-12\\\\Annotated_ALLSFA_21\\\\Unseen pdf\"\n",
    "os.chdir(path)\n",
    "filename=[]\n",
    "new_data1 = []\n",
    "for file in glob.glob(\"*.pdf\"):\n",
    "    os.chdir(path)\n",
    "    print(file)\n",
    "    all_text = \" \"\n",
    "    with pdfplumber.open(file) as pdf:\n",
    "            for pdf_page in pdf.pages:\n",
    "                single_page_text = pdf_page.extract_text()\n",
    "                if len(single_page_text) > 30:\n",
    "                    all_text = all_text + single_page_text\n",
    "                    all_text = clean(all_text)\n",
    "                    file_temp = file.replace('.pdf','.txt')\n",
    "                    with open(file_temp,'w',encoding='UTF-8') as f:\n",
    "                        f.write(all_text)\n",
    "                else:\n",
    "                    pass\n",
    "    new_data1.append(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313eb52a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2968/2070924955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_ner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mnlp_ner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mdic_checking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tag\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textData' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def check_expiration_date(effective_date):\n",
    "    match_temp = re.search('\\d\\d\\d\\d', effective_date)\n",
    "    match_temp1=re.search('\\d\\d$',effective_date)\n",
    "    if match_temp is not None:\n",
    "        find_string = str(match_temp.group())\n",
    "        replace_string = str(int(match_temp.group()) + 2)\n",
    "        return effective_date.replace(find_string,replace_string)\n",
    "\n",
    "    else:\n",
    "        if match_temp1 is not None:\n",
    "            find_string = str(match_temp1.group())\n",
    "            replace_string = str(int(match_temp1.group()) + 2)\n",
    "            return effective_date.replace(find_string, replace_string)\n",
    "\n",
    "def check_parameter(dic,TERM_LENGTH_CHECK,EXPIRATION_DATE_CHECK):\n",
    "    if TERM_LENGTH_CHECK == True:\n",
    "        dic_temp = [\"TERM_LENGTH\", False]\n",
    "\n",
    "        for lab1 in dic['label']:\n",
    "            if lab1 == dic_temp[0]:\n",
    "                dic_temp = [\"TERM_LENGTH\", True]\n",
    "                break\n",
    "        if dic_temp[1] == False:\n",
    "            dic[\"tag\"].append(\"2 years\")\n",
    "            dic[\"label\"].append(dic_temp[0])\n",
    "    if EXPIRATION_DATE_CHECK == True:\n",
    "        dic_temp1 = [\"EXPIRATION_DATE\", False]\n",
    "\n",
    "        for lab1 in dic['label']:\n",
    "            if lab1 == dic_temp1[0]:\n",
    "                dic_temp = [\"TERM_LENGTH\", True]\n",
    "                break\n",
    "        if dic_temp1[1] == False:\n",
    "            for tag, label in zip(dic[\"tag\"], dic['label']):\n",
    "                if label == \"EFFECTIVE_DATE\":\n",
    "                    dic[\"tag\"].append(check_expiration_date(tag))\n",
    "                    dic[\"label\"].append(dic_temp1[0])\n",
    "\n",
    "    return dic\n",
    "\n",
    "\n",
    "doc = nlp_ner(textData)\n",
    "nlp_ner.max_length = len(textData)+100\n",
    "dic_checking = {\"tag\":[],\"label\":[]}\n",
    "for token in doc.ents:\n",
    "    dic_checking[\"tag\"].append(token.text)\n",
    "    dic_checking[\"label\"].append(token.label_)\n",
    "dic_checking = check_parameter(dic_checking,TERM_LENGTH_CHECK=True,EXPIRATION_DATE_CHECK=True)\n",
    "#for tag,label in zip(dic_checking[\"tag\"],dic_checking['label']):\n",
    "    #print(str(tag)+\"----------\"+label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0ebd7",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c46d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\A1058564\\Anaconda3\\lib\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.2.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'D:\\OneDrive_2022-01-12\\training_data\\sm_output_47')\n",
    "nlp_ner = spacy.load(r\".\\model-best\") #load the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafff403",
   "metadata": {},
   "source": [
    "## Results in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3a54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reports_from_text(location, nlp):\n",
    "    for file in [f for f in os.listdir(location) if f.lower().endswith('.txt')]:\n",
    "        text_file_location=\"{}{}\".format(location,file)\n",
    "        with open(text_file_location,encoding='UTF-8') as f:\n",
    "            data = f.read()\n",
    "            nlp_ner.max_length = len(textData)+100\n",
    "            dic_checking = {\"tag\":[],\"label\":[]}\n",
    "            for token in doc.ents:\n",
    "                \n",
    "    # print(token.text+'----'+token.label_)\n",
    "                dic_checking[\"tag\"].append(token.text)\n",
    "                dic_checking[\"label\"].append(token.label_)\n",
    "        #nlp.max_length = len(data)+100\n",
    "        #doc = nlp(data)\n",
    "        #dict1 = {'Label': [], 'Output': []}\n",
    "        #for token in doc.ents:\n",
    "            #dict1['Label'].append(token.label_)\n",
    "            #dict1['Output'].append(token.text)\n",
    "            dic_checking = check_parameter(dic_checking,TERM_LENGTH_CHECK=True,EXPIRATION_DATE_CHECK=True)\n",
    "            df = pd.DataFrame(dic_checking)\n",
    "            file_temp = file.replace('.txt','.csv')\n",
    "            file_temp = \"{}{}\".format(location,file_temp)\n",
    "            df.to_csv(file_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e1a935",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2968/1025770127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_reports_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\\\OneDrive_2022-01-12\\\\21_ALLSFA\\\\'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp_ner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2968/2421376348.py\u001b[0m in \u001b[0;36mcreate_reports_from_text\u001b[1;34m(location, nlp)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_file_location\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mnlp_ner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mdic_checking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tag\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textData' is not defined"
     ]
    }
   ],
   "source": [
    "create_reports_from_text('D:\\\\OneDrive_2022-01-12\\\\21_ALLSFA\\\\', nlp_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd65479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 12:49:46.546892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-10 12:49:46.546943: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Usage: python -m spacy evaluate [OPTIONS] MODEL DATA_PATH\n",
      "Try 'python -m spacy evaluate --help' for help.\n",
      "\n",
      "Error: Invalid value for 'DATA_PATH': Path './test.spacy' does not exist.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'D:\\OneDrive_2022-01-12\\training_data\\sm_output_47')\n",
    "!spacy evaluate ./output/model-best ./test.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7fc46a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: D:\\Users\\A1058564\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy-model-en_core_web_sm\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _anaconda_depends  pkgs/main/win-64::_anaconda_depends-2021.11-py39_0\n",
      "  beautifulsoup4     conda-forge/noarch::beautifulsoup4-4.10.0-pyha770c72_0\n",
      "  catalogue          conda-forge/win-64::catalogue-2.0.6-py39hcbf5309_1\n",
      "  chardet            conda-forge/win-64::chardet-4.0.0-py39hcbf5309_2\n",
      "  cymem              conda-forge/win-64::cymem-2.0.6-py39h415ef7b_2\n",
      "  cython-blis        conda-forge/win-64::cython-blis-0.7.6-py39h5d4886f_0\n",
      "  filelock           conda-forge/noarch::filelock-3.6.0-pyhd8ed1ab_0\n",
      "  langcodes          conda-forge/noarch::langcodes-3.3.0-pyhd8ed1ab_0\n",
      "  murmurhash         conda-forge/win-64::murmurhash-1.0.6-py39h415ef7b_2\n",
      "  numpy-base         pkgs/main/win-64::numpy-base-1.20.3-py39hc2deb75_0\n",
      "  pandas             conda-forge/win-64::pandas-1.4.1-py39h2e25243_0\n",
      "  pathy              conda-forge/noarch::pathy-0.6.1-pyhd8ed1ab_0\n",
      "  pip                conda-forge/noarch::pip-22.0.4-pyhd8ed1ab_0\n",
      "  preshed            conda-forge/win-64::preshed-3.0.6-py39h415ef7b_1\n",
      "  pydantic           conda-forge/win-64::pydantic-1.8.2-py39hb82d6ee_2\n",
      "  setuptools         conda-forge/win-64::setuptools-60.9.3-py39hcbf5309_0\n",
      "  shellingham        conda-forge/noarch::shellingham-1.4.0-pyh44b312d_0\n",
      "  smart_open         conda-forge/noarch::smart_open-5.2.1-pyhd8ed1ab_0\n",
      "  spacy              conda-forge/win-64::spacy-3.2.2-py39hefe7e4c_0\n",
      "  spacy-legacy       conda-forge/noarch::spacy-legacy-3.0.9-pyhd8ed1ab_0\n",
      "  spacy-loggers      conda-forge/noarch::spacy-loggers-1.0.1-pyhd8ed1ab_0\n",
      "  spacy-model-en_co~ conda-forge/noarch::spacy-model-en_core_web_sm-3.2.0-pyhd8ed1ab_0\n",
      "  srsly              conda-forge/win-64::srsly-2.4.2-py39h415ef7b_1\n",
      "  thinc              conda-forge/win-64::thinc-8.0.14-py39hefe7e4c_0\n",
      "  typer              conda-forge/noarch::typer-0.4.0-pyhd8ed1ab_0\n",
      "  typing-extensions  pkgs/main/noarch::typing-extensions-3.10.0.2-hd3eb1b0_0\n",
      "  wasabi             conda-forge/noarch::wasabi-0.9.0-pyhd8ed1ab_0\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/win-64::anaconda==2021.11=py39_0\n",
      "  - defaults/win-64::anaconda-client==1.9.0=py39haa95532_0\n",
      "  - defaults/win-64::anaconda-navigator==2.1.1=py39_0\n",
      "  - defaults/noarch::anaconda-project==0.10.1=pyhd3eb1b0_0\n",
      "  - defaults/win-64::astroid==2.6.6=py39haa95532_0\n",
      "  - defaults/win-64::astropy==4.3.1=py39hc7d831d_0\n",
      "  - defaults/noarch::backports.functools_lru_cache==1.6.4=pyhd3eb1b0_0\n",
      "  - defaults/win-64::bcrypt==3.2.0=py39h196d8e1_0\n",
      "  - defaults/noarch::binaryornot==0.4.4=pyhd3eb1b0_1\n",
      "  - defaults/win-64::bkcharts==0.2=py39haa95532_0\n",
      "  - defaults/win-64::bokeh==2.4.1=py39haa95532_0\n",
      "  - defaults/win-64::bottleneck==1.3.2=py39h7cc1a96_1\n",
      "  - defaults/win-64::clyent==1.2.2=py39haa95532_1\n",
      "  - defaults/win-64::conda==4.11.0=py39haa95532_0\n",
      "  - defaults/win-64::conda-build==3.21.6=py39haa95532_0\n",
      "  - defaults/noarch::conda-pack==0.6.0=pyhd3eb1b0_0\n",
      "  - defaults/noarch::conda-repo-cli==1.0.4=pyhd3eb1b0_0\n",
      "  - defaults/noarch::conda-token==0.3.0=pyhd3eb1b0_0\n",
      "  - defaults/noarch::conda-verify==3.4.2=py_1\n",
      "  - defaults/noarch::cookiecutter==1.7.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::daal4py==2021.3.0=py39h757b272_0\n",
      "  - defaults/noarch::dask==2021.10.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::distributed==2021.10.0=py39haa95532_0\n",
      "  - defaults/noarch::flake8==3.9.2=pyhd3eb1b0_0\n",
      "  - defaults/noarch::flask==1.1.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::gevent==21.8.0=py39h2bbff1b_1\n",
      "  - defaults/win-64::h5py==3.2.1=py39h3de5c98_0\n",
      "  - defaults/win-64::imagecodecs==2021.8.26=py39ha1f97ea_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0\n",
      "  xlrd               conda-forge/noarch::xlrd-2.0.1-pyhd8ed1ab_3\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2021.10.8-py39haa9~ --> conda-forge::certifi-2021.10.8-py39hcbf5309_1\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2021.10.26~ --> conda-forge::ca-certificates-2021.10.8-h5b45459_0\n",
      "  openssl              pkgs/main::openssl-1.1.1l-h2bbff1b_0 --> conda-forge::openssl-1.1.1l-h8ffe710_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  anaconda                                   2021.11-py39_0 --> custom-py39_1\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - defaults/noarch::imageio==2.9.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::ipykernel==6.4.1=py39haa95532_1\n",
      "  - defaults/win-64::ipython==7.29.0=py39hd4e2768_0\n",
      "  - defaults/noarch::ipywidgets==7.6.5=pyhd3eb1b0_1\n",
      "  - defaults/noarch::jinja2==2.11.3=pyhd3eb1b0_0\n",
      "  - defaults/noarch::jinja2-time==0.2.0=pyhd3eb1b0_2\n",
      "  - defaults/noarch::jsonschema==3.2.0=pyhd3eb1b0_2\n",
      "  - defaults/win-64::jupyter==1.0.0=py39haa95532_7\n",
      "  - defaults/noarch::jupyterlab==3.2.1=pyhd3eb1b0_1\n",
      "  - defaults/noarch::jupyterlab_pygments==0.1.2=py_0\n",
      "  - defaults/noarch::jupyterlab_server==2.8.2=pyhd3eb1b0_0\n",
      "  - defaults/noarch::jupyter_console==6.4.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::jupyter_server==1.4.1=py39haa95532_0\n",
      "  - defaults/win-64::matplotlib==3.4.3=py39haa95532_0\n",
      "  - defaults/win-64::matplotlib-base==3.4.3=py39h49ac443_0\n",
      "  - defaults/win-64::mkl_fft==1.3.1=py39h277e83a_0\n",
      "  - defaults/win-64::mkl_random==1.2.2=py39hf11a4ad_0\n",
      "  - defaults/noarch::nbclassic==0.2.6=pyhd3eb1b0_0\n",
      "  - defaults/noarch::nbclient==0.5.3=pyhd3eb1b0_0\n",
      "  - defaults/win-64::nbconvert==6.1.0=py39haa95532_0\n",
      "  - defaults/noarch::nbformat==5.1.3=pyhd3eb1b0_0\n",
      "  - defaults/win-64::notebook==6.4.5=py39haa95532_0\n",
      "  - defaults/win-64::numba==0.54.1=py39hf11a4ad_0\n",
      "  - defaults/win-64::numexpr==2.7.3=py39hb80d3ca_1\n",
      "  - defaults/win-64::numpy==1.20.3=py39ha4e8547_0\n",
      "  - defaults/noarch::numpydoc==1.1.0=pyhd3eb1b0_1\n",
      "  - defaults/noarch::paramiko==2.7.2=py_0\n",
      "  - defaults/win-64::patsy==0.5.2=py39haa95532_0\n",
      "  - defaults/win-64::pyerfa==2.0.0=py39h2bbff1b_0\n",
      "  - defaults/noarch::pygments==2.10.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::pylint==2.9.6=py39haa95532_1\n",
      "  - defaults/noarch::pyls-spyder==0.4.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::pytables==3.6.1=py39h56d22b6_1\n",
      "  - defaults/noarch::python-lsp-black==1.0.0=pyhd3eb1b0_0\n",
      "  - defaults/noarch::python-lsp-server==1.2.4=pyhd3eb1b0_0\n",
      "  - defaults/win-64::pywavelets==1.1.1=py39h080aedc_4\n",
      "  - defaults/noarch::qtconsole==5.1.1=pyhd3eb1b0_0\n",
      "  - defaults/win-64::scikit-image==0.18.3=py39hf11a4ad_0\n",
      "  - defaults/win-64::scikit-learn==0.24.2=py39hf11a4ad_1\n",
      "  - defaults/win-64::scikit-learn-intelex==2021.3.0=py39haa95532_0\n",
      "  - defaults/win-64::scipy==1.7.1=py39hbe87c03_2\n",
      "  - defaults/noarch::seaborn==0.11.2=pyhd3eb1b0_0\n",
      "  - defaults/noarch::soupsieve==2.2.1=pyhd3eb1b0_0\n",
      "  - defaults/noarch::sphinx==4.2.0=pyhd3eb1b0_1\n",
      "  - defaults/win-64::spyder==5.1.5=py39haa95532_1\n",
      "  - defaults/win-64::spyder-kernels==2.1.3=py39haa95532_0\n",
      "  - defaults/win-64::statsmodels==0.12.2=py39h2bbff1b_0\n",
      "  - defaults/noarch::tifffile==2021.7.2=pyhd3eb1b0_2\n",
      "  - defaults/win-64::widgetsnbextension==3.5.1=py39haa95532_0\n",
      "  - defaults/win-64::zope.event==4.5.0=py39haa95532_0\n",
      "  - defaults/win-64::zope.interface==5.4.0=py39h2bbff1b_0\n",
      "  - defaults/win-64::_ipyw_jlab_nb_ext_conf==0.1.0=py39haa95532_0\n",
      "\n",
      "RemoveError: 'setuptools' is a dependency of conda and cannot be removed from\n",
      "conda's operating environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy-model-en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c546a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1ccf408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_path DATA_PATH --test_data_path_to_save TEST_DATA_PATH_TO_SAVE --train_ratio\n",
      "                             TRAIN_RATIO\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_path, --test_data_path_to_save, --train_ratio\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def get_files_from_folder(path):\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    return np.asarray(files)\n",
    "\n",
    "def main(path_to_data, path_to_test_data, train_ratio):\n",
    "    # get dirs\n",
    "    _, dirs, _ = next(os.walk(path_to_data))\n",
    "\n",
    "    # calculates how many train data per class\n",
    "    data_counter_per_class = np.zeros((len(dirs)))\n",
    "    for i in range(len(dirs)):\n",
    "        path = os.path.join(path_to_data, dirs[i])\n",
    "        files = get_files_from_folder(path)\n",
    "        data_counter_per_class[i] = len(files)\n",
    "    test_counter = np.round(data_counter_per_class * (1 - train_ratio))\n",
    "\n",
    "    # transfers files\n",
    "    for i in range(len(dirs)):\n",
    "        path_to_original = os.path.join(path_to_data, dirs[i])\n",
    "        path_to_save = os.path.join(path_to_test_data, dirs[i])\n",
    "\n",
    "        #creates dir\n",
    "        if not os.path.exists(path_to_save):\n",
    "            os.makedirs(path_to_save)\n",
    "        files = get_files_from_folder(path_to_original)\n",
    "        # moves data\n",
    "        for j in range(int(test_counter[i])):\n",
    "            dst = os.path.join(path_to_save, files[j])\n",
    "            src = os.path.join(path_to_original, files[j])\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Dataset divider\")\n",
    "    parser.add_argument(\"--data_path\", required=True,\n",
    "    help=\"Path to data\")\n",
    "    parser.add_argument(\"--test_data_path_to_save\", required=True,\n",
    "    help=\"Path to test data where to save\")\n",
    "    parser.add_argument(\"--train_ratio\", required=True,\n",
    "    help=\"Train ratio - 0.7 means splitting data in 70 % train and 30 % test\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args.data_path, args.test_data_path_to_save, float(args.train_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb1390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Get a list of the files\n",
    "# 2.Randomize the files\n",
    "# 3.Split files into training and testing sets\n",
    "# 4.Do the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e711b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Get a list of the files\n",
    "import os\n",
    "\n",
    "def get_file_list_from_dir(datadir):\n",
    "    all_files = os.listdir(os.path.abspath(datadir))\n",
    "    data_files = list(filter(lambda file: file.endswith('.json'), all_files))\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6513c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Randomize the files\n",
    "import random\n",
    "\n",
    "def randomize_files(file_list):\n",
    "    shuffled = random.sample(list_of_files, len(list_of_files))\n",
    "    return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877dc536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Split files into training and testing sets\n",
    "\n",
    "from math import floor\n",
    "\n",
    "def get_training_and_testing_sets(file_list):\n",
    "    split = 0.7\n",
    "    split_index = floor(len(file_list) * split)\n",
    "    training = file_list[:split_index]\n",
    "    testing = file_list[split_index:]\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94550001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_function(datadir = 'D:\\200_Annotated_Files\\Consolidated'):\n",
    "    \n",
    "    list_of_files = get_file_list_from_dir(datadir)\n",
    "    suffled_data = randomize_files(list_of_files)\n",
    "    training, testing = get_training_and_testing_sets(suffled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e40ea9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp1 =  spacy.blank('en')  # load a new spacy model\n",
    "nlp2 =  spacy.blank('en')\n",
    "db1 = DocBin() # create a DocBin object\n",
    "db2 = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "286538db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42823858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list_from_dir(datadir):\n",
    "    all_files = os.listdir(os.path.abspath(datadir))\n",
    "    data_files = list(filter(lambda file: file.endswith('.json'), all_files))\n",
    "    return data_files\n",
    "\n",
    "def randomize_files(list_of_files):\n",
    "    shuffled = random.sample(list_of_files, len(list_of_files))\n",
    "    return shuffled\n",
    "\n",
    "def get_training_and_testing_sets(file_list):\n",
    "    split = 0.8\n",
    "    split_index = floor(len(file_list) * split)\n",
    "    training = file_list[:split_index]\n",
    "    testing = file_list[split_index:]\n",
    "    return training, testing\n",
    "\n",
    "def ml_function(datadir='D:\\\\75\\\\'):\n",
    "    list_of_files = get_file_list_from_dir(datadir)\n",
    "    suffled_data = randomize_files(list_of_files)\n",
    "    training, testing = get_training_and_testing_sets(suffled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba832b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_file_list_from_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22436/4196219181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mml_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'D:\\\\75\\\\'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file_list_from_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_file_list_from_dir'"
     ]
    }
   ],
   "source": [
    "ml_function(datadir='D:\\\\75\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c55e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class random:\n",
    "    \n",
    "    def __init__(self, datadir):\n",
    "        self.datadir = datadir\n",
    "        \n",
    "    \n",
    "    def get_file_list_from_dir(self):\n",
    "        all_files = os.listdir(os.path.abspath(self.datadir))\n",
    "        data_files = list(filter(lambda file: file.endswith('.json'), all_files))\n",
    "        return data_files\n",
    "\n",
    "    def randomize_files(self):\n",
    "        shuffled = random.sample(list_of_files, len(list_of_files))\n",
    "        return shuffled\n",
    "\n",
    "    def get_training_and_testing_sets(self):\n",
    "        split = 0.8\n",
    "        split_index = floor(len(file_list) * split)\n",
    "        training = file_list[:split_index]\n",
    "        testing = file_list[split_index:]\n",
    "        return training, testing\n",
    "\n",
    "    def ml_function(self):\n",
    "        list_of_files = get_file_list_from_dir(self.datadir)\n",
    "        suffled_data = randomize_files(list_of_files)\n",
    "        training, testing = get_training_and_testing_sets(suffled_data)\n",
    "        print(training,testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac68251",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj =random('D:\\\\200_Annotated_Files\\\\Consolidated\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf82686",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'random' has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9540/1036580928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9540/104010081.py\u001b[0m in \u001b[0;36mml_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mml_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlist_of_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_file_list_from_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0msuffled_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandomize_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_and_testing_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffled_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9540/3479922386.py\u001b[0m in \u001b[0;36mrandomize_files\u001b[1;34m(file_list)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrandomize_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mshuffled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mshuffled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'random' has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "obj.ml_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0260402b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22436/1940848448.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\RANDOMSAMPLES'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mTRAIN_DATA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir('D:\\113_files\\1\\')\n",
    "for filename in training:\n",
    "    f = open(filename,encoding='UTF-8')\n",
    "    TRAIN_DATA = json.load(f)\n",
    "\n",
    "    for text, annot in tqdm(TRAIN_DATA['annotations']):\n",
    "        doc1 = nlp1.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc1.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc1.ents = ents \n",
    "        db1.add(doc1)\n",
    "\n",
    "os.chdir(r'D:\\OneDrive_2022-01-12\\training_data')\n",
    "db1.to_disk(\"./train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d46accfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 56.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 40.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 37.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.86it/s]\n"
     ]
    }
   ],
   "source": [
    "os.chdir('D:\\RANDOMSAMPLES')\n",
    "for filename in testing:\n",
    "    f = open(filename,encoding='UTF-8')\n",
    "    TEST_DATA = json.load(f)\n",
    "\n",
    "    for text, annot in tqdm(TEST_DATA['annotations']):\n",
    "        doc2 = nlp2.make_doc(text) \n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc2.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc2.ents = ents \n",
    "        db2.add(doc2)\n",
    "\n",
    "os.chdir(r'D:\\OneDrive_2022-01-12\\training_data')\n",
    "db2.to_disk(\"./test.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "670eb28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sepire MSA FINAL.json',\n",
       " 'Revelation Software_EULA & Schedules_033007.json',\n",
       " 'Zelis Healthcare-Master Services and License Agreement-Master Service Agreement-06-26-2018 (1).json',\n",
       " 'SUPP-00000785 - SOW- 09-23-2021 - 12-31-2021.json',\n",
       " \"TRADUCTIONS SERGE BELAIR INC-Master Service Agreement-Master Services Agreement-5-01-2017'.json\",\n",
       " 'Premier International Enterprises, Inc.json',\n",
       " 'ON24 INC MSA.json',\n",
       " 'WILLIS TOWERS WATSON US LLC MSA.json',\n",
       " 'NorthgateArinsoMSAandSOW_signed6-22-10.json',\n",
       " 'MSA signature page.json',\n",
       " 'Topic SRL - MSA.json',\n",
       " 'WIPRO_MSA_2018-04-09_001.json',\n",
       " 'NorthgateArinso - e-know - Web Services Agreement FULLY EXECUTED 9-30-10.json',\n",
       " 'S-001338 - SFA - 09-10-2021 - 09-09-2023.json',\n",
       " 'WORKFORCE OUTSOURCE SERVICES INC-Master Service Agreement-Consulting Agreement-10-25-2017.json',\n",
       " 'SUNGARD AVAILABILITY SERVICES LP-Master Service Agreement-Subscription Services Agreement-7-01-2018.json',\n",
       " 'WINDWARD STUDIOS INC-Master Software License Agreement-License Agreement-11-01-2013.json',\n",
       " 'PARASOFT CORPORATION-Master Software License Agreement-Master Software License Agreement-5-04-2016.json',\n",
       " 'SUPPLIERGATEWAY LLC MSA.json',\n",
       " 'OPEN TEXT-Master Service Agreement-Standard Services Agreement-5-30-2013.json',\n",
       " 'TRAVIS SOFTWARE CORPORATION-Master Service Agreement-Travis Software Corp Specialized Services Agreement-4-29-2016.json',\n",
       " 'SUPP-00002602 - CURIOUS LION SFA SUPPLIER FRAMEWORK AGREEMENT MSA.json',\n",
       " 'TERRY GROUP MSA.json',\n",
       " 'Outsolve, LLC-Master Service Agreement-Master Services Agreement-12-08-2017.json',\n",
       " 'ASAP_Staffing_-_SFA.json',\n",
       " 'SFA_Age_of_Human_202100807.docx.json',\n",
       " 'Scanner App-Scanbot SDK License Agreement-6-20-2018_Executed copy.json',\n",
       " '0000000 - SFA - 09-22-2021 - 09-21-2023.json',\n",
       " 'PARKPLACE_MSA_2021-08-09_001.json',\n",
       " 'SEACOM LLC-Master Service Agreement-Master Services Agreement-5-01-2017 (1).json',\n",
       " 'Wipro LLC-Master Software License Agreement-Software License Agreement-09-01-2018.json',\n",
       " 'REED GROUP LTD-Master Service Agreement-Leave Management Services Agreement-1-01-2018.json',\n",
       " '1044256-NGA-MSA-110618-110621.json',\n",
       " 'TELEPLUS INC-Master Service Agreement-Master Services Agreement-1-01-2015.json',\n",
       " 'Tenable MSA.json',\n",
       " 'Optiv-Alight-MSA.json']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(\"datafile.txt\", \"rb\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:50]\n",
    "test_data = data[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83e612d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   53.68 \n",
      "NER R   35.42 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 12:15:03.820698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-19 12:15:03.820753: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "D:\\Users\\A1058564\\Anaconda3\\lib\\site-packages\\spacy\\util.py:833: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.2.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER F   42.68 \n",
      "SPEED   17091 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                               P       R       F\n",
      "SUPPLIER_NAME              57.14   19.05   28.57\n",
      "SUPPLIER_STREET_ADDRESS    57.14   30.77   40.00\n",
      "NET_PAYMENT_TERM           75.00   81.82   78.26\n",
      "NOTICE_PERIOD              37.04   50.00   42.55\n",
      "ALIGHT_ENTITY              50.00   64.71   56.41\n",
      "SUPPLIER_ZIP_CODE          50.00    7.69   13.33\n",
      "SUPPLIER_CITY              60.00   21.43   31.58\n",
      "SUPPLIER_STATE             25.00    9.09   13.33\n",
      "TERM_LENGTH               100.00   42.86   60.00\n",
      "EFFECTIVE_DATE            100.00   38.46   55.56\n",
      "SUPPLIER_COUNTRY            0.00    0.00    0.00\n",
      "SUPPLIER_STREET_ADRESS      0.00    0.00    0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'D:\\OneDrive_2022-01-12\\training_data')\n",
    "!spacy evaluate ./output/model-best ./test2.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5204a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
